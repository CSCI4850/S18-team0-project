{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Abstract"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "    Our goal is to find similiarities in authors based on their style of writing. Given text based data such as amazon reviews, books, tweets, and even emails, we would like to find features to represent unique authors. The way the text is encoded can directly affect the results. Word2Vec and Doc2Vec are very popular models are used to enocode text into similarity vectors. They are normally 300-wide vectors containing double values normally ranging from -3 to 3. Even past these methods, reversing the input text can affect how the network learns. Primarily we used three nueral networks for testing. On each we tested the normal enocodings and the reveresed encodings. We hope that using neiral nets with these encodings, that is can accurately detect similarity between different authors based on their unique writing styles. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "    Text-based classification has been used for many different things. A lot of it has to do with semantics checking and the like. We are trying to find unique stamps for an individual author based on they way that they write. We originally surmised that Word2Vec along with Doc2Vec could find these similarities and build a unique profile for an author on a per document basis. The nueral networks would try to use this unique vector data to build a profile for an individual author. We have referenced a paper [reference here] using tweets from twitter as input. The paper is uaing ngrams for single words, making the window or filter surrounding characters instead of groups of words. Our approach will be to use word groups instead of looking at individual characters in a single word or groups of words. Our custom model uses Word2Vec on individal words in a document and sums the corrosponding word vectors to fake a Doc2Vec model. Word2Vec uses (CBOW) while Doc2Vec uses (DBOW), so there is a major distinction here. We want to know if these distinctions will matter when being trained in various neural networks. Sampling also could have different effects. In our models, some runs have normal training data, while the others has been oversampled to try and improve training and testing accuracy.\n",
    "    In terms of nueral networks, massive nets proved to be very slow at training and unsuccessful in giving good similarity predictions,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Methods"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "    We used a combination of gensim and spacy to get our encodings for Word2Vec and Doc2Vec. We have used Google's pretrained model and even trained our own custom models to produce the word vectors. Our custom model was much smaller than Googles. We first organize the documents (representing a user and their review) and place the auther and the review seperated by a tab on a single line. This is done for every review to make parsing easier. Sorting is done by author to make splitting the data into training and test easier. We then load up the two models and run our data through them. The output is the author and their encoded vector of words for that current document seperated by a tab on a single line for all documents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discussion and Conclusion"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "    In general, our tests along with our neural networks, proved to be mostly unsuccessful at building a unique feature-set for classifying indivudal authors. The best result that was acheived (referenced above) was only acheivable using 3 authors. As the number of authors increased, the accuracy decreased sharply. Overfitting was a huge problem proving to be our biggest downfall. Along with that, Word2Vec and Doc2Vec are not the best at picking up stylist differences in authorship. They adept at finding semeantic similarites though."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
